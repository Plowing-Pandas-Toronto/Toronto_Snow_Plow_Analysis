{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79334b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploratory analysis of json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e89d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Configure Notebook\n",
    "#for plots to be inline\n",
    "%matplotlib inline \n",
    "#for auto_complete \n",
    "%config Completer.use_jedi = False \n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d62908e-31a3-48a7-9eb6-8768e9b069a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def file_parser():\n",
    "    \"\"\"This function will import every json file inside the folders that are inside 'data_in_path'.\n",
    "    It will then concatenate every file into a huge dataframe, to be saved in the data_out_path.\n",
    "    The notebook running this function must be in the root directory.\n",
    "    \"\"\"\n",
    "    print('parsing the files now')\n",
    "    data_in_path = r'src/Snow Feb 17'\n",
    "    data_out_path = r'src/17_parsed_plow_df'\n",
    "    \n",
    "    #read the existence of the out directory\n",
    "    if os.path.isdir(data_out_path) == False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), data_out_path))\n",
    "    \n",
    "    #listing all files in the folder and ordering them ascending.\n",
    "    files = [f for f in os.listdir(os.path.join(os.getcwd(), data_in_path)) if '164' in f]\n",
    "    files.sort()\n",
    "    \n",
    "    #creating dummy DF to be filled with values\n",
    "    full_df = pd.DataFrame()\n",
    "    i=0\n",
    "    for folder in files:\n",
    "        i=i+1\n",
    "        #this print will keep re-writing itself\n",
    "        print ('Folder {}, file {} of {} - - - - {:.2f}% completed'.format(folder,i,len(files), 100*i/len(files)), end=\"\\r\")\n",
    "        for file in os.listdir(os.path.join(os.getcwd(), data_in_path,folder)):\n",
    "            #print(file)\n",
    "            json_file = json.load(open(os.path.join(os.getcwd(), data_in_path, folder, file)))\n",
    "\n",
    "            if (('features' in json_file) and (len(json_file['features'])>1)):\n",
    "                if 'status' in json_file['features'][0]['properties']:\n",
    "                    temp = gpd.read_file(os.path.join(os.getcwd(), data_in_path,folder,file))\n",
    "                    full_df = gpd.GeoDataFrame(pd.concat([full_df, temp]), crs=temp.crs)\n",
    "        #if i == 15:\n",
    "         #   full_df.to_file(os.path.join(os.getcwd(), data_out_path,r'streets_incomplete.geojson'), driver='GeoJSON')\n",
    "          #  break\n",
    "            \n",
    "    #saving the huge DF as a geojson. can also change this into a shp file\n",
    "    #full_df.to_file(os.path.join(os.getcwd(), data_out_path,r'streets_complete.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95779f9-2b8d-4df3-91e4-ef74add9aa9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#alternative function to file_parser. This parser will drop columns and remove duplicates each iteration, keeping the final DF smaller.\n",
    "def file_parser_cleaner():\n",
    "    \"\"\"This function will import every json file inside the folders that are inside 'data_in_path'.\n",
    "    It will then concatenate every file into a huge dataframe, to be saved in the data_out_path.\n",
    "    The notebook running this function must be in the root directory.\n",
    "    \"\"\"\n",
    "    print('parsing the files now')\n",
    "    data_in_path = r'src/Snow Feb 17'\n",
    "    data_out_path = r'src/17_parsed_plow_df'\n",
    "    \n",
    "    #read the existence of the out directory\n",
    "    if os.path.isdir(data_out_path) == False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), data_out_path))\n",
    "    \n",
    "    #listing all files in the folder and ordering them ascending.\n",
    "    files = [f for f in os.listdir(os.path.join(os.getcwd(), data_in_path)) if '164' in f]\n",
    "    files.sort()\n",
    "    \n",
    "    #creating dummy DF to be filled with values\n",
    "    full_df = pd.DataFrame()\n",
    "    i=0\n",
    "    for folder in files:\n",
    "        i=i+1\n",
    "        #this print will keep re-writing itself\n",
    "        print ('Folder {}, file {} of {} - - - - {:.2f}% completed'.format(folder,i,len(files), 100*i/len(files)), end=\"\\r\")\n",
    "        for file in os.listdir(os.path.join(os.getcwd(), data_in_path,folder)):\n",
    "            #print(file)\n",
    "            json_file = json.load(open(os.path.join(os.getcwd(), data_in_path, folder, file)))\n",
    "\n",
    "            if (('features' in json_file) and (len(json_file['features'])>1)):\n",
    "                if 'status' in json_file['features'][0]['properties']:\n",
    "                    temp = gpd.read_file(os.path.join(os.getcwd(), data_in_path,folder,file))\n",
    "                    temp.sort_values(by='completetime', ascending=True, inplace=True)\n",
    "                    temp = temp[temp['status'] == '1'] #keeping only streets cleaned in the last 4 hours.\n",
    "                    temp.drop(columns={'district', \n",
    "                    'OBJECTID',\n",
    "                    'routeno',\n",
    "                    'unqueid',\n",
    "                   'line',\n",
    "                   'receivedtime'}, inplace=True)\n",
    "                    \n",
    "                    full_df = gpd.GeoDataFrame(pd.concat([full_df, temp]), crs=temp.crs)\n",
    "                    full_df.drop_duplicates(inplace=True)\n",
    "        \n",
    "    full_df.to_file(os.path.join(os.getcwd(), data_out_path,r'streets_complete.geojson'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9349e89-65b0-4536-99c1-f3607729671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the DF\n",
    "def data_cleaner():\n",
    "    \"\"\" This function will take the DF saved as a GeoJson in the df_path and will clean it\n",
    "    \"\"\"\n",
    "    print('cleaning the plow data', end=\"\\n\")\n",
    "    df_path = r'src/17_parsed_plow_df'\n",
    "    file_name = r'streets_complete.geojson' #For debugging purposes. Otherwise use 'streets_complete.geojson' when running.\n",
    "    data_out_path = r'src/streets_cleaned'\n",
    "    file_out_name = r'17_streets_cleaned.geojson' #will keep overwriting\n",
    "    \n",
    "    #renaming columns of interest\n",
    "    old_columns=['Shape__Length','name_neigh']\n",
    "    renamed_columns=['length','neighbourhood']\n",
    "    renaming=dict(zip(old_columns,renamed_columns))\n",
    "    \n",
    "    df = gpd.read_file(os.path.join(os.getcwd(),df_path,file_name))\n",
    "    df['completed_time'] = pd.to_datetime(df['completetime'], unit='ms') #I need to add a datetime column first to give the 'unit' argument\n",
    "    df.sort_values(by='completed_time', ascending=True, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.index = pd.DatetimeIndex(df['completed_time'])\n",
    "    df = df[df['status'] == '1'] #keeping only streets cleaned in the last 4 hours.\n",
    "    \n",
    "    #dropping useless columns\n",
    "    df.drop(columns={'index', 'completetime',\n",
    "                    'completed_time'}, inplace=True)\n",
    "                     \n",
    "    \n",
    "    #adding crs info to df\n",
    "    df.crs = {'init':'epsg:4326'}\n",
    "    df = df.to_crs('EPSG:4326')\n",
    "    \n",
    "    #many lines will be duplicated, so dropping them here\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    df = df.rename(columns=renaming)\n",
    "    \n",
    "    if os.path.isdir(data_out_path) == False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), data_out_path))\n",
    "        \n",
    "    #Saving the cleaned neighbourhoods DF\n",
    "    df.to_file(os.path.join(os.getcwd(), data_out_path, file_out_name), driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03a99ca-62dc-4715-8275-ecd17b2a2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports neighbourhoods shapefile and cleans it up. saves just the necessary columns. no calculations.\n",
    "def neighbourhoods_prep():\n",
    "    \"\"\"This code will read the original Toronto Open Data dataset, clean it up to only keep the columns in 'columns_keep' \n",
    "    and save it into the 'data_out_path' folder.\n",
    "    \"\"\"\n",
    "    print('preparing the neighbourhood data')\n",
    "    data_path = r'src/Neighbourhoods'\n",
    "    file_name = r'Neighbourhoods.shp'\n",
    "    data_out_path = r'src/17_Neighbourhoods_cleaned'\n",
    "    file_out_name = r'Neighbourhoods_cleaned.shp'\n",
    "    \n",
    "    #selecting columns to keep, and what to rename them\n",
    "    columns_keep = ['FIELD_7','FIELD_2','FIELD_14','geometry']\n",
    "    rename=['name_neigh','id','area','geometry']\n",
    "    renaming=dict(zip(columns_keep,rename))\n",
    "    \n",
    "    #Importing the neighbourhoods shapefile\n",
    "    print('reading file')\n",
    "    neighbourhoods = gpd.read_file(os.path.join(os.getcwd(), data_path, file_name))\n",
    "    neighbourhoods = neighbourhoods[columns_keep]\n",
    "    neighbourhoods = neighbourhoods.rename(columns=renaming)\n",
    "    \n",
    "    #removing the ID number from the neighbourhood name column\n",
    "    print('renaming columns')\n",
    "    neighbourhoods['name_neigh'] = neighbourhoods['name_neigh'].str.replace('[\\d+()]', '', regex=True)\n",
    "    \n",
    "    if os.path.isdir(data_out_path) == False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), data_out_path))\n",
    "        \n",
    "    #Saving the cleaned neighbourhoods DF\n",
    "    print('saving file')\n",
    "    neighbourhoods.to_file(os.path.join(os.getcwd(), data_out_path, file_out_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f98fbd6-32f8-4f96-aa99-3a34e39a7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locates each cleaned street into a neighbourhood, using centroid and a spatial join\n",
    "#it will output a multiindex df showing the length of streets cleaned within the last hour per neighbourhood.\n",
    "def locate_streets():\n",
    "    from shapely.geometry import LineString\n",
    "    print('locating cleaned streets inside each neighbourhood')\n",
    "\n",
    "    data_path = r'src/streets_cleaned'\n",
    "    file_name = r'17_streets_cleaned.geojson'\n",
    "    \n",
    "    neigh_data_path = r'src/17_Neighbourhoods_cleaned'\n",
    "    neigh_file_name = r'Neighbourhoods_cleaned.shp'\n",
    "    data_out_path = r'src/17_joined_streets'\n",
    "    data_out_file = r'joined_streets.geojson'\n",
    "    \n",
    "    #loading geojson and correcting datetime column\n",
    "    print('reading streets')\n",
    "    streets_cleaned = gpd.read_file(os.path.join(os.getcwd(), data_path, file_name))\n",
    "    print('converting to datetime and dropping columns')\n",
    "    streets_cleaned['completed_time'] = pd.to_datetime(streets_cleaned['completed_time'])\n",
    "    streets_cleaned.index = pd.DatetimeIndex(streets_cleaned['completed_time'])\n",
    "    streets_cleaned.drop(columns={'completed_time'}, inplace=True)\n",
    "    \n",
    "    #Making sure that geometry displays linestring\n",
    "    print('applying geometry')\n",
    "    streets_cleaned['geometry'] = streets_cleaned.geometry.apply(LineString)\n",
    "    \n",
    "    #creating a centroid column\n",
    "    print('locating based on centroid')\n",
    "    streets_cleaned['centroid'] = streets_cleaned['geometry'].centroid\n",
    "    \n",
    "    #loading the cleaned neighbourhoods file, and saving the geometry which will be used later\n",
    "    print('loading neighbourhoods file')\n",
    "    cleaned_neighbourhoods = gpd.read_file(os.path.join(os.getcwd(), neigh_data_path, neigh_file_name))\n",
    "    cleaned_neighbourhoods['savedgeom'] = cleaned_neighbourhoods.geometry\n",
    "    \n",
    "    #localizing each cleaned street inside a neighbourhood by spatial join\n",
    "    print('performing spatial join')\n",
    "    joined_df = gpd.sjoin(left_df=streets_cleaned, \n",
    "                      right_df=cleaned_neighbourhoods, \n",
    "                     op = 'within').drop(columns={'index_right','id','area'})\n",
    "    joined_df = joined_df.rename(columns={'name_neigh':'neighbourhood'})\n",
    "    \n",
    "    #aggregating into groups, and having multiindex\n",
    "    print('aggregating into multiindex')\n",
    "    grouped_streets = joined_df.groupby([pd.Grouper(freq='H'), 'neighbourhood', 'route_name']).agg(routetype = ('routetype', 'first'),\n",
    "                                               length = ('length', 'sum'),\n",
    "                                             geometry = ('savedgeom', 'first'))\n",
    "    \n",
    "    #after joining, it is a simple dataframe. converting into a geodataframe now:\n",
    "    print('converting into gpd')\n",
    "    grouped_streets = gpd.GeoDataFrame(grouped_streets, geometry='geometry')\n",
    "    grouped_streets.crs = {'init':'epsg:4326'}\n",
    "    grouped_streets = grouped_streets.to_crs('EPSG:4326')\n",
    "    \n",
    "    #saving df\n",
    "    print('saving df')\n",
    "    if os.path.isdir(data_out_path) == False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), data_out_path))\n",
    "    grouped_streets.to_file(os.path.join(os.getcwd(), data_out_path, data_out_file), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b7735f-76cf-4d8e-af9a-7e86a3a2c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('starting main script')\n",
    "    file_parser_cleaner()\n",
    "    data_cleaner()\n",
    "    neighbourhoods_prep()\n",
    "    locate_streets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bacb5b64-35e2-42c8-97c8-62c46f5de8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting main script\n",
      "parsing the files now\n",
      "cleaning the plow datae 3 of 3 - - - - 100.00% completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CIV1498_37/lib/python3.7/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing the neighbourhood data\n",
      "reading file\n",
      "renaming columns\n",
      "saving file\n",
      "locating cleaned streets inside each neighbourhood\n",
      "reading streets\n",
      "converting to datetime and dropping columns\n",
      "applying geometry\n",
      "locating based on centroid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CIV1498_37/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading neighbourhoods file\n",
      "performing spatial join\n",
      "aggregating into multiindex\n",
      "converting into gpd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CIV1498_37/lib/python3.7/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving df\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
